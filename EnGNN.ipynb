{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:28:49.767247Z",
     "start_time": "2024-05-08T08:28:48.157752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "49a6575f03ce754c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Equivariant Graph Neural Network\n",
    "We have a graph $G = (V, E)$ with nodes $v_i \\in V$ with $N$ number of nodes and edges $e_{ij} \\in E$, each node has a feature vector $h_i \\in \\mathbb{R}^k$ and an n-dimensional coordinate vector $x_i \\in \\mathbb{R}^n$.\n",
    "\n",
    "We would like to preserve equivariance to rotations, translations and permutations.\n"
   ],
   "id": "12083670294c75f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Equivariant Graph Convolutional Layer (EGCL)\n",
    "- **Input**: node embeddings **$ h^l = \\{ h^l_0, ..., h^l_{N - 1}\\} $**, edge information: **$E$**, coordinate embeddings **$x^l = \\{ x^l_0, ..., x^l_{N - 1} \\}$**\n",
    "\n",
    "$$\n",
    "    \\{ H^l, E, X^l \\}\n",
    "$$\n",
    "\n",
    "- **Output**: a transformation on the node embeddings and the coordinate embeddings\n",
    "\n",
    "$$\n",
    "    h^{l+1}, x^{l+1} = EGCL[h^l, x^l, E]\n",
    "$$"
   ],
   "id": "bb3aa81584ee5a2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The equations that define this layer are the following:\n",
    "$$\n",
    "    m_{ij} = \\phi_e(h^l_i, h^l_j, || x^l_i - x^l_j ||^2, a_{ij})\n",
    "$$\n",
    "$$\n",
    "    m_i = \\sum_{j \\in \\mathit{N}(i)} m_{ij}\n",
    "$$\n",
    "$$\n",
    "    x^{l+1}_i = x^{l}_i + C \\sum_{j \\neq i} (x^l_i - x^l_j) \\phi_x(m_{ij})\n",
    "$$\n",
    "$$\n",
    "    h^{l+i}_i = \\phi_h(h^l_i, m_i)\n",
    "$$"
   ],
   "id": "2115a58fcdf971ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Following the paper the edge attributes are just the edge values $a_{ij} = e_{ij}$ but other attribute could also be included. $C = \\frac{1}{N-1}$ ( minus one because we have of course $j \\neq i $) \n",
    "**It's left to be defined $\\phi_e, \\phi_x, \\phi_h$**. Following the Appendices:\n",
    "- Edge Function **$\\phi_e$**:  $\\phi_e$ is a two layers MLP with two Swish non-linearities: Input −→ {LinearLayer() −→ Swish()\n",
    "−→ LinearLayer() −→ Swish() } −→ Output.\n",
    "- Coordinate Function **$\\phi_x$**:  $\\phi_x$ ( consists of a two layers MLP with one non-linearity: $m_{ij}$ −→ {LinearLayer() −→\n",
    "Swish() −→ LinearLayer() } −→ Output\n",
    "- Node Function **$\\phi_h$**: $\\phi_h$ consists of a two layers MLP with one non-linearity and a residual connection:\n",
    "$[h^l_i, m_i]$ −→ {LinearLayer() −→ Swish() −→ LinearLayer() −→ Addition($h^l_i$) } −→ $h^{l+1}$"
   ],
   "id": "2f58968c585f17e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice that the paper **doesn't define how the inputs are combined together for the edge function**. Considering the nature of inputs it's reasonable to define the function in this way:\n",
    "$$\n",
    "    \\hat{h}_{ij} = a_{ij} || x_i - x_j ||^2 * [h_i, h_j] \\\\\n",
    "    m_{ij} = \\phi_e (\\hat{h}_{ij})\n",
    "$$ \n",
    "Where $a_{ij}$ and $|| x_i - x_j ||^2$ multiplied together act as weight for the concatenation of the two embeddings. Of course others formulations are possible.\n",
    "Let's first define the Swish activation function:"
   ],
   "id": "80010480028d7b0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ],
   "id": "13b32d61c7cb8fc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Edge function\n",
    "The architecture of the model is the one defined in the Appendix C"
   ],
   "id": "c9d27ca7e1d3cd5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Phi_e(nn.Module):\n",
    "    def __init__(self, n_input_features, n_hidden_features, n_output_features):\n",
    "        super(Phi_e, self).__init__()\n",
    "        n_input_features = n_input_features*2\n",
    "        self.model = nn.Sequential(nn.Linear(n_input_features, n_hidden_features),\n",
    "                                   Swish(),\n",
    "                                   nn.Linear(n_hidden_features, n_output_features),\n",
    "                                   Swish())\n",
    "    '''\n",
    "        We cannot assume that the concatenation of [h_i, h_j] must produce the same results than [h_j, h_i]\n",
    "    '''\n",
    "    def forward(self, Edges, Coordinates, Embeddings):\n",
    "        ## Define the input [ a_{ij} || - ||^, h_i, h_j]\n",
    "        ## Compute the distance matrix\n",
    "        distance_matrix = torch.cdist(Coordinates, Coordinates, p=2)\n",
    "        ## Combine the values of the weights\n",
    "        w = Edges * distance_matrix\n",
    "        ## Produce all the combinations of [h_i, h_j] also when j = i (it's possible to speed up this procedure)\n",
    "        def concat_embed(embedding_a, embedding_b):\n",
    "            return torch.cat((embedding_a, embedding_b))\n",
    "        H = torch.vmap(func=torch.vmap(concat_embed, in_dims=(None, 0)), in_dims=(0, None))(Embeddings, Embeddings)\n",
    "        H_hat = w[:, :, None] * H\n",
    "        n_nodes = w.shape[0]\n",
    "        H_hat = torch.flatten(H_hat, start_dim=0, end_dim=1)\n",
    "        M = self.model(H_hat)\n",
    "        M_unflattened = torch.unflatten(M, 0, (n_nodes, n_nodes, -1)).squeeze()\n",
    "        return M_unflattened"
   ],
   "id": "995d4087d520b5fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Coordinate Function\n",
    "The architecture of the model is the one defined in the Appendix C"
   ],
   "id": "29c9b9c6242d2478"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Phi_x(nn.Module):\n",
    "    def __init__(self, n_input_features, n_hidden_features, n_output_features):\n",
    "        super(Phi_x, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(n_input_features, n_hidden_features),\n",
    "                                   Swish(),\n",
    "                                   nn.Linear(n_hidden_features, n_output_features))\n",
    "\n",
    "    def forward(self, M):\n",
    "        return self.model(M)"
   ],
   "id": "b691d22dc4e34034"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Node Function\n",
    "The architecture of the model is the one defined in the Appendix C"
   ],
   "id": "607e1576ba1062b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Phi_h(nn.Module):\n",
    "    def __init__(self, n_input_features, n_hidden_features, n_output_features):\n",
    "        super(Phi_h, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(n_input_features, n_hidden_features),\n",
    "                                   Swish(),\n",
    "                                   nn.Linear(n_hidden_features, n_output_features))\n",
    "\n",
    "    def forward(self, H, M):\n",
    "        input = torch.concatenate([H, M], dim=1)\n",
    "        out = self.model(input)\n",
    "        out = out + H\n",
    "        return out"
   ],
   "id": "7f4e6296fa1fd1b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Equivariant Graph Convolutional Layer\n",
    "Here we also implemented the possibility to have velocities for the nodes. As reported\n",
    "$$\n",
    "    v^{l+1}_i = \\phi_v(h^l_i)v^{l}_i + C \\sum_{j \\neq i} (x^l_i - x^l_j) \\phi_x(m_{ij}) \\\\\n",
    "    x^{l+1}_i = x^l_i + v^{l+1}_i\n",
    "$$"
   ],
   "id": "5853128875fa641b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EGCL(nn.Module):\n",
    "    def __init__(self, n_input_features, n_hidden_features, n_output_features, with_velocity):\n",
    "        super(EGCL, self).__init__()\n",
    "\n",
    "        self.with_velocity = with_velocity\n",
    "        if with_velocity:\n",
    "            self.phi_v = nn.Sequential(nn.Linear(n_input_features, n_hidden_features),\n",
    "                                       Swish(),\n",
    "                                       nn.Linear(n_hidden_features, 1))\n",
    "        self.phi_e = Phi_e(n_input_features=n_hidden_features,\n",
    "                           n_hidden_features=n_hidden_features,\n",
    "                           n_output_features=n_hidden_features)\n",
    "\n",
    "        self.phi_x = Phi_x(n_input_features=n_hidden_features,\n",
    "                           n_hidden_features=n_hidden_features,\n",
    "                           n_output_features=1)\n",
    "\n",
    "        self.phi_h = Phi_h(n_input_features=n_hidden_features*2,\n",
    "                           n_hidden_features = n_hidden_features,\n",
    "                           n_output_features=n_output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_velocity:\n",
    "            edges, coordinates, embeddings, velocities = x\n",
    "        else:\n",
    "            edges, coordinates, embeddings = x\n",
    "        M_edge = self.phi_e(edges, coordinates, embeddings)\n",
    "        n_nodes = M_edge.shape[0]\n",
    "        difference_position_fun = lambda x_i, x_j: x_i - x_j\n",
    "        differences_of_positions = torch.vmap(func=torch.vmap(difference_position_fun, in_dims=(None, 0)), in_dims=(0, None))(coordinates, coordinates)\n",
    "\n",
    "        M_edge_flatten = torch.flatten(M_edge, start_dim=0, end_dim=1)\n",
    "        M_x = self.phi_x(M_edge_flatten)\n",
    "        M_x_unflattened = torch.unflatten(M_x, 0, (n_nodes, n_nodes, -1)).squeeze()\n",
    "\n",
    "        partials = (M_x_unflattened * (torch.ones((n_nodes, n_nodes), device=M_edge.device) - torch.eye(n=n_nodes, device=M_edge.device))).unsqueeze(-1)\n",
    "        del M_x_unflattened\n",
    "        if self.with_velocity:\n",
    "            new_velocities = self.phi_v(embeddings)*velocities + (1/(n_nodes - 1))* torch.sum(differences_of_positions * partials, dim=0)\n",
    "            new_coordinates = coordinates + new_velocities\n",
    "        else:\n",
    "            new_coordinates = coordinates + (1/(n_nodes - 1))* torch.sum(differences_of_positions * partials, dim=0)\n",
    "\n",
    "        del differences_of_positions\n",
    "        M_aggregated = torch.sum(M_edge * edges.unsqueeze(-1), dim=1)\n",
    "        new_embeddings = self.phi_h(embeddings, M_aggregated)\n",
    "        if self.with_velocity:\n",
    "            return edges, new_coordinates, new_embeddings, new_velocities\n",
    "        else:\n",
    "            return edges, new_coordinates, new_embeddings"
   ],
   "id": "71ef6f502a1ccf67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## E(n) Equivariant Graph Neural Network\n",
    "Here we report the architectural structure of the model for the EGNN in experiments for the QM9 dataset:\n",
    "- _\"Our EGNN consists of 7 layers.[...]\"_ \n",
    "- _\"Finally, the output of our EGNN hL is forwarded through a two layers MLP that acts  ode-wise, a sum pooling operation and another two layers MLP that maps the averaged embedding to the predicted property value, more formally: hL −→ {Linear() −→ Swish() −→ Linear() −→ Sum-Pooling() −→ Linear() −→ Swish() −→ Linear} −→ Property. The number of hidden features for all model hidden layers is 128\"_"
   ],
   "id": "85c42f1588725945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EnGNN(nn.Module):\n",
    "    def __init__(self, n_input_features, n_hidden_features, n_output_features, with_velocity=False):\n",
    "        super(EnGNN, self).__init__()\n",
    "        self.with_velocity = with_velocity\n",
    "        self.embed = nn.Linear(n_input_features, n_hidden_features)\n",
    "        self.model = nn.Sequential(EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity),\n",
    "                                   EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity),\n",
    "                                   EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity),\n",
    "                                   EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity),\n",
    "                                   EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity),\n",
    "                                   EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity),\n",
    "                                   EGCL(n_hidden_features, n_hidden_features, n_hidden_features, with_velocity))\n",
    "        self.head_model_1 = nn.Sequential(nn.Linear(n_hidden_features, n_hidden_features),\n",
    "                                        Swish(),\n",
    "                                        nn.Linear(n_hidden_features, n_hidden_features))\n",
    "        self.head_model_2 = nn.Sequential(nn.Linear(n_hidden_features, n_hidden_features),\n",
    "                                        Swish(),\n",
    "                                        nn.Linear(n_hidden_features, n_output_features))\n",
    "\n",
    "        self.edge_inferring_model = nn.Sequential(nn.Linear(n_hidden_features, 1),\n",
    "                                                  nn.Sigmoid())\n",
    "\n",
    "    def forward(self, Edges, Coordinates, Embeddings, batch_pointer, velocities=None):\n",
    "        Embeddings = self.embed(Embeddings)\n",
    "        if self.with_velocity:\n",
    "            edges, coordinates, embeddings, velocities = self.model((Edges, Coordinates, Embeddings, velocities))\n",
    "        else:\n",
    "            edges, coordinates, embeddings = self.model((Edges, Coordinates, Embeddings))\n",
    "        embeddings = self.head_model_1(embeddings)\n",
    "        ### Aggregation with pointer not implemented yet from torch geometric\n",
    "        ranges = batch_pointer.unfold(0, 2, 1)\n",
    "        embeddings_list = []\n",
    "        for indeces in ranges:\n",
    "            embeddings_list.append(torch.sum(embeddings[indeces[0]:indeces[1]], dim=0 ))\n",
    "        ####################################################################à\n",
    "        embeddings = torch.stack(embeddings_list)\n",
    "        out = self.head_model_2(embeddings)\n",
    "        return out"
   ],
   "id": "f6f322ed64470821"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notes \n",
    "The article unfortunately leaves some implementation details not totally clear that we have to infer:\n",
    "- The definition of how the input of $\\phi_e$ are combined is not defined:\n",
    "$$\n",
    "\\phi_e(h^l_i, h^l_j, || x^l_i - x^l_j ||^2, a_{ij})\n",
    "$$\n",
    "- The article uses $\\phi_{\\text{inf}}$ to infer the edges of the graph starting from a fully connected one. The article also report that $\\phi_{\\text{inf}}(m_{ij})$ so depending on the output of $\\phi_e$ that is part of a EGCL. This leaves ambiguity if $\\phi_{\\text{inf}}$ infer the edges at each layer starting from a situation of a fully connected; or if the edges are inferred only for the first layer. This is not clear."
   ],
   "id": "29d3111ba60bbfe5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
